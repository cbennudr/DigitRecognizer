{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "'''\r\n",
    "https://www.sitepoint.com/keras-digit-recognition-tutorial/\r\n",
    "https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5\r\n",
    "\r\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from keras.datasets import mnist\r\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "print(y_train[2])\r\n",
    "plt.imshow(X_train[5], cmap='Greys')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "4\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1612f2f6a60>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    },
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "print(X_train.shape)\r\n",
    "print(X_test.shape)\r\n",
    "'''\r\n",
    "(60000, 28, 28) - 60,000 datapoints and each is 28x28 pixels\r\n",
    "(10000, 28, 28)\r\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\n(60000, 28, 28) - 60,000 datapoints and each is 28x28 pixels\\n(10000, 28, 28)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "#keras needs each image to be formatted like (M x N x 1)\r\n",
    "'''\r\n",
    "Next, we need to reshape our dataset inputs (X_train and X_test) to the shape \r\n",
    "that our model expects when we train the model. The first number is the number\r\n",
    "of images (60,000 for X_train and 10,000 for X_test). Then comes the shape of \r\n",
    "each image (28x28). The last number is 1, which signifies that the images are greyscale.\r\n",
    "'''\r\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\r\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\r\n",
    "\r\n",
    "#normalize image data\r\n",
    "X_train = X_train/255\r\n",
    "X_test = X_test/255"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "# make dummies\r\n",
    "from tensorflow.keras.utils import to_categorical\r\n",
    "\r\n",
    "y_train = to_categorical(y_train)\r\n",
    "y_test = to_categorical(y_test)\r\n",
    "print(y_train.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(60000, 10)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "#cretae the NN\r\n",
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\r\n",
    "\r\n",
    "model = Sequential() #create model obj\r\n",
    "\r\n",
    "#add layers\r\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation='relu', input_shape=(28, 28, 1)))\r\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\r\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
    "#to avoid overfitting, neurons and their connections are randomly dropped during training\r\n",
    "model.add(Dropout(0.25)) #25% of the neurons are dropped\r\n",
    "#flattening layer which converts previoud hidden layer into a 1D array\r\n",
    "model.add(Flatten())\r\n",
    "model.add(Dense(128, activation='relu'))\r\n",
    "model.add(Dropout(.5))\r\n",
    "    #softmax is used when the data is going to be classified into a number of predecided classes\r\n",
    "model.add(Dense(numDigits, activation='softmax')) \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "#Specify a loss function, an optimizer function, and a metric to assess model performance\r\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "#train the model\r\n",
    "    #batch size is the # of samples that will go through the NN at a time\r\n",
    "batch_size = 128\r\n",
    "    #epochs are the number of times the entire dataset is passed forward and backward through the NN\r\n",
    "epochs = 10\r\n",
    "\r\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \r\n",
    "                    verbose=1, validation_data=(X_test, y_test))\r\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\r\n",
    "\r\n",
    "print('Test loss: ', score[0])\r\n",
    "print('Test accuracy: ', score[1])\r\n",
    "model.save(\"digit_recognizer_model.h5\")\r\n",
    "'''\r\n",
    "Test loss:  0.029256155714392662\r\n",
    "Test accuracy:  0.9916999936103821\r\n",
    "'''"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "469/469 [==============================] - 48s 101ms/step - loss: 0.2362 - accuracy: 0.9286 - val_loss: 0.0498 - val_accuracy: 0.9849\n",
      "Epoch 2/10\n",
      "469/469 [==============================] - 49s 105ms/step - loss: 0.0838 - accuracy: 0.9751 - val_loss: 0.0398 - val_accuracy: 0.9876\n",
      "Epoch 3/10\n",
      "469/469 [==============================] - 48s 102ms/step - loss: 0.0633 - accuracy: 0.9808 - val_loss: 0.0346 - val_accuracy: 0.9895\n",
      "Epoch 4/10\n",
      "469/469 [==============================] - 46s 99ms/step - loss: 0.0545 - accuracy: 0.9832 - val_loss: 0.0315 - val_accuracy: 0.9896\n",
      "Epoch 5/10\n",
      "469/469 [==============================] - 46s 98ms/step - loss: 0.0467 - accuracy: 0.9855 - val_loss: 0.0294 - val_accuracy: 0.9911\n",
      "Epoch 6/10\n",
      "469/469 [==============================] - 46s 98ms/step - loss: 0.0374 - accuracy: 0.9880 - val_loss: 0.0274 - val_accuracy: 0.9916\n",
      "Epoch 7/10\n",
      "469/469 [==============================] - 46s 98ms/step - loss: 0.0340 - accuracy: 0.9891 - val_loss: 0.0292 - val_accuracy: 0.9913\n",
      "Epoch 8/10\n",
      "469/469 [==============================] - 46s 99ms/step - loss: 0.0318 - accuracy: 0.9899 - val_loss: 0.0279 - val_accuracy: 0.9919\n",
      "Epoch 9/10\n",
      "469/469 [==============================] - 46s 98ms/step - loss: 0.0281 - accuracy: 0.9913 - val_loss: 0.0292 - val_accuracy: 0.9909\n",
      "Epoch 10/10\n",
      "469/469 [==============================] - 46s 97ms/step - loss: 0.0264 - accuracy: 0.9914 - val_loss: 0.0293 - val_accuracy: 0.9917\n",
      "Test loss:  0.029256155714392662\n",
      "Test accuracy:  0.9916999936103821\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experimentation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "from keras.models import Sequential\r\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\r\n",
    "\r\n",
    "model2 = Sequential() #create model obj\r\n",
    "model2.add(Conv2D(64, kernel_size=(3,3), activation='relu', input_shape=(28, 28, 1)))\r\n",
    "model2.add(Conv2D(32, kernel_size=(3,3), activation='relu'))\r\n",
    "model2.add(Flatten())\r\n",
    "model2.add(Dense(10, activation='softmax')) #output layer needs 10 nodes becasue 10 possible values (0-9)\r\n",
    "\r\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
    "\r\n",
    "batch_size = 128\r\n",
    "epochs = 3\r\n",
    "model2.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \r\n",
    "                    verbose=1, validation_data=(X_test, y_test))\r\n",
    "score = model2.evaluate(X_test, y_test, verbose=0)\r\n",
    "\r\n",
    "print('Test loss: ', score[0])\r\n",
    "print('Test accuracy: ', score[1])\r\n",
    "#.980300"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n",
      "469/469 [==============================] - 56s 118ms/step - loss: 0.1911 - accuracy: 0.9449 - val_loss: 0.0606 - val_accuracy: 0.9825\n",
      "Epoch 2/3\n",
      "469/469 [==============================] - 50s 107ms/step - loss: 0.0594 - accuracy: 0.9818 - val_loss: 0.0526 - val_accuracy: 0.9821\n",
      "Epoch 3/3\n",
      "469/469 [==============================] - 53s 113ms/step - loss: 0.0423 - accuracy: 0.9872 - val_loss: 0.0450 - val_accuracy: 0.9842\n",
      "Test loss:  0.04500932991504669\n",
      "Test accuracy:  0.9842000007629395\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "print(model2.predict(X_test[:5]))\r\n",
    "print(y_test[:5])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.3967463e-10 6.2563774e-11 7.1414078e-08 1.2779966e-06 6.3699701e-12\n",
      "  3.9764393e-11 1.4785572e-17 9.9999642e-01 7.6988318e-09 2.0932166e-06]\n",
      " [3.9160337e-08 2.4013059e-07 9.9999774e-01 5.5104419e-09 2.5800515e-14\n",
      "  3.4745090e-10 2.0078314e-06 1.0946730e-15 1.5166405e-08 9.0706671e-14]\n",
      " [5.2220889e-07 9.9912149e-01 4.3478834e-05 1.5230291e-06 1.9862209e-04\n",
      "  8.9471951e-07 1.2195417e-06 3.9125560e-04 2.4016046e-04 8.3304661e-07]\n",
      " [9.9995983e-01 2.8098074e-10 1.9047553e-06 2.4124146e-08 1.6609349e-07\n",
      "  7.8244433e-08 2.7926531e-05 2.2500037e-06 7.6621212e-08 7.8096309e-06]\n",
      " [4.1265751e-09 1.1455385e-10 7.2430650e-10 5.4029242e-10 9.9997950e-01\n",
      "  1.2715740e-12 2.0860307e-10 4.1227302e-08 4.0939252e-08 2.0484038e-05]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "d3f5693a50ba4f77d76de97bf5031daef64d83771d220ae2fba131fd9d09e1ec"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}